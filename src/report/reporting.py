# %%
import sys

from datetime import datetime
from pathlib import Path
import pandas as pd
import numpy as np

import missingno as msno

from etl import etl_pipe as etl
from etl import geo_pipe as geo
from checks import NaptanCheck

# %%
timestr = datetime.now().strftime("%Y_%m_%d")
dl_home = Path(Path.home() / "Downloads/")


# %%
def print_column_info(df):
    """[summary] pretty prints pandas dataframe data.
    Doesn't need a return statemnet.
    Args:
        df ([type]): [description]
    """
    template = "%-8s %-30s %s"
    print(template % ("Type", "Column Name", "Example Value"))
    print("-" * 53)
    # TODO - makes sure it returns valid entries for examples.
    for c in df.columns:
        print(template % (df[c].dtype, c, df[c].iloc[1]))


#  %%
def write_basic_log_file(message):
    """[summary] writes a very basic log file.

    Args:
        message ([str]): [the string of ]

    Raises:
        ke: [description]
        e: [description]
        e: [description]

    Returns:
        [type]: [description]
    """
    # see a reference to the orgiianl standard output
    original_stdout = sys.stdout
    # write things.
    # path and files for log files
    log_file = Path(f"{dl_home}/Naptan_Error_Reports_{timestr}_naptan_process.log")
    with open(log_file, "a+") as f:
        sys.stdout = f
        print(message)
        sys.stdout = original_stdout


# %%


def report_failing_nodes(
    gdf,
    test_name,
    failed_nodes,
    check_warning_level="medium",
    check_geographical_level="stops",
):
    """[Returns float indicating the total number of nodes failing a given test.]

    Arguments:
        gdf {[geopandasdataframe]} -- [the master naptan dataframe]
        testName {[str]} -- [name of the given test that called this method.]
        failed_nodes {[pandas dataframe]} -- [the failed node dataframe
        generated by the check been performed is passed to the rep.]
        check_warning_level (str, optional): [description]. Defaults to 'medium'.
        check_geographical_level (str, optional): [description]. Defaults to 'stops'.
    Returns:
        [Error report files] -- [A percentage figure.]
    """

    try:
        # check if the tests returned a full dataframe if anything
        # if it does not then just print that it's fine.
        if failed_nodes.shape[0] == 0 or failed_nodes.empty:
            message = f"{test_name} has passed successfully."
            write_basic_log_file(message=message)

        elif not failed_nodes.empty:
            # get the name of the reported areas from the failed nodes.
            fail_area = failed_nodes.AreaName.iloc[0]
            print(f"{test_name} has failed for {fail_area}.")
            # we get the length of the total naptan gdf here
            # we make a percentage of the failed nodes.
            result = failed_nodes.shape[0] / gdf.shape[0]
            error_percent = (
                f""" {timestr},"""
                f"""Check {test_name} failed nodes, """
                f""" {result:0.3f}% of all Naptan, """
                f""" {failed_nodes.shape[0]} failed this check."""
            )
            # set the report folder
            report_folder = Path(f"{dl_home}/Naptan_Error_Reports_{timestr}/")
            Path(report_folder).mkdir(parents=True, exist_ok=True)
            print(f"{report_folder} error folder has being created.")
            # set the file names and set the names.
            node_stats = Path(report_folder, f"{fail_area}_Failed_Check_Stats.csv")
            #
            with open(node_stats, "a+") as text_file:
                print(error_percent, file=text_file)
            #
            report_file = Path(report_folder, f"{fail_area}_{test_name}_Warnings.csv")
            #
            with open(report_file, "a+") as f:
                failed_nodes.to_csv(
                    f, index=False, encoding="utf-8", header=f.tell() == 0
                )

    except ZeroDivisionError as zero:
        raise (f"{zero}")
        print(f"The {test_name} has not returned any failing naptan nodes.")
        pass

    except FileExistsError:
        print(f"The {timestr} report file has been created.")

    except FileNotFoundError:
        print("This should not happen.")

    except KeyError as ke:
        raise ke
        sys.exit(f"{ke} a key error has been encountered")

    except Exception as e:
        raise e
        sys.exit(f"{e} Report creation failed")


# %%


def plot_missing_data(df, sample_rate):
    """[summary]

    Args:
        df ([type]): [description]
        sample_rate ([type]): [description]

    Returns:
        [type]: [description]
    """
    fig = msno.matrix(df.sample(sample_rate))
    plot = fig.get_figure()
    # plot = fig.savefig('Missing Data Matrix.png')
    # fig.save_figure(format='bmp')
    return plot


# %%
def visualise_missing_counts(df, sample_rate):
    """[summary] Provides a visualise matrix view and bar chart of missing
    naptan values.  Creates a null data map of naptan geodata, using missing no.

    Raises:
        e: [description]

    Returns:
        [type] -- [description]
    """
    # TODO -> return this data as an output.
    dfcol = list(df.columns.values)
    output = []
    for i in dfcol:
        # write column length.
        output.append(len(df[i]))
        output.append(len(df[i].unique()))
        # count nan/nulls
        nullvalues = df.isnull().sum(axis=0)
        # count of zero values
        zerovalues = df[df == 0].count(axis=0)
        # get percentage of nan rows.
        percent_nan = df[i].isna().mean().round(4) * 100
        # print(nullvalues, zerovalues, percent_nan)
        nullDataMap = msno.matrix(df.sample(sample_rate))
        # nullDataMap.save_figure(format='.bmp')
    return nullDataMap


# %%
def list_unique_values(df, column_name):
    """[summary]  takes a dataframe and returns a listing the unique values
    therein.
    Returns:
        [list] -- [description]
    """
    lst = df[column_name].drop_duplicates()
    df_col_unique = lst.tolist()
    return df_col_unique


# %%


def naptan_admin_area_stats(gdf_admin_area):
    """[summary] a method that takes a naptan loclaity gdf and returns a
    a measurement

    Args:
        gdf_admin_area (str): [Must be a string received from ].

    Raises:
        e: [description]
    """
    try:
        # load the nptg gazette.
        localities = etl.naptan_gazette_localities()
        # group by counts of the locality.
        local_stats = gdf_admin_area.value_counts(
            subset=["LocalityName", "NptgLocalityCode", "AreaName"]
        ).reset_index()
        # drop cabbage column in place
        local_stats.drop(0, axis=1, inplace=True)
        # from nptg get the centroid locations of each locality
        local_centroid_stats = local_stats.merge(
            localities[["NptgLocalityCode", "Gazette_geometry"]], on="NptgLocalityCode"
        )
        # get total count of the localities in the admin area.
        # record the stats
        local_centroid_stats.to_csv(
            f"{gdf_admin_area}_stats.csv", encoding="utf-8", sep=",", header=True
        )
        print(f"{gdf_admin_area} stats file has been generated.")
    except Exception as e:
        raise e


# %%
def report_recent_naptan_upload(gdf_locality, num_recent: int) -> int:
    """[summary]  returns the last n uploads for the given administrative area
    and localities affected.

    Args:
        gdf_locality ([type]): [description]
        num_recent (int): [description]

    Returns:
        int: [description]
    """
    # TODO - make this function.
    #  get the naptan data
    # filter to the most recent entries, filter by creation data
    # create a pandas series timeframe
    today = pd.Timestamp(datetime.today()).floor("D")
    # split by diff on creation date by upload time on server.
    diff = today - gdf_locality.CreationDate
    # loc by timedelta max index values of the num recent.
    indexmax = diff[(diff < pd.to_timedelta(10))].idxmax()
    recent_naptan_entries = gdf_locality.iloc[[indexmax]]
    # get the latest, specified number
    # TODO - gets us a the top 10
    topten = gdf_locality["CreationDate"].drop_duplicates().nlargest(num_recent)
    return recent_naptan_entries


# %%
def measure_naptan_groups(gdf, naptan_column_name="LocalityName"):
    """[summary]  this function measures the number of groups present within
    the given geodataframe, when groupbed the [default LocalityNames]

    Args:
        gdf ([type]): [description]
        naptan_column_name ([type]): [description]

    Returns:
        [type]: [description]
    """

    # filter dataset to bare minimum needed.
    naptan_column_name = "LocalityName"
    gdf2 = gdf[["AreaName", "LocalityName", "geometry", "Longitude", "Latitude"]]
    # reconvert to geo
    gdf3 = geo.calculate_naptan_geometry(gdf2)
    gdf3 = gdf3.drop(["Longitude", "Latitude"], axis=1)
    # groupby the naptan column type.
    groups = gdf3.groupby(naptan_column_name)
    # gets us the number of values in the geometry column.
    counts = gdf3.groupby(naptan_column_name).size().reset_index()
    counts.columns = [f"{naptan_column_name}", f"Size_{naptan_column_name}"]

    return counts, groups


# %%
def get_number_column_entries(gdf, naptan_column_name="AreaName"):
    """[summary] returns a size count of the number of entries in the column
    specified. e.g. counts how main points are in geometry field.

    Args:
        gdf ([type]): [description]
        naptan_column_name (str, optional): [description]. Defaults to 'AreaName'.

    Returns:
        [type]: [description]
    """
    # groupby by size transform column, gives you the number of points in the
    #  geometry column.
    gdf["size"] = gdf.groupby([f"{naptan_column_name}"]).transform(np.size)
    return gdf
